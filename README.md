# qwen2.5-codegen-lora

Qwen2.5-Coder LoRA Fine-Tuning on CodeGen-Deep-5K & CodeGen-Diverse-5K

Bu proje, Qwen/Qwen2.5-Coder-1.5B-Instruct modelinin
CodeGen-Deep-5K ve CodeGen-Diverse-5K datasetleri Ã¼zerinde
QLoRA yÃ¶ntemi ile fine-tune edilmesini, checkpoint deÄŸerlendirmelerini ve
en iyi model seÃ§imini iÃ§erir.

Proje AmacÄ±

Kod Ã¼retim modelleri iÃ§in derin reasoning ve farklÄ± problem kapsama alanÄ± sunan iki dataset Ã¼zerinde ince ayarlÄ± (LoRA) eÄŸitim gerÃ§ekleÅŸtirmek
EÄŸitim sÃ¼recinde oluÅŸan birden fazla checkpointâ€™i deÄŸerlendirip en iyi performans veren modeli seÃ§mek
TÃ¼m pipelineâ€™Ä± GitHub & HuggingFace Ã¼zerinde dÃ¶kÃ¼mleyerek araÅŸtÄ±rma/proje teslim hedeflerini karÅŸÄ±lamak

Dataset AÃ§Ä±klamasÄ±

CodeGen-Deep-5K
1000 farklÄ± problem Ã— her biri iÃ§in 5 farklÄ± Ã§Ã¶zÃ¼m
Reasoning + final Ã§Ã¶zÃ¼m iÃ§erir
Derin dÃ¼ÅŸÃ¼nme gerektiren problemlerde modelin adÄ±m adÄ±m mantÄ±k kurmasÄ±nÄ± Ã¶ÄŸretir
CodeGen-Diverse-5K
5000 benzersiz problem
Her problem iÃ§in 1 Ã§Ã¶zÃ¼m
GeniÅŸ problem Ã§eÅŸitliliÄŸi â†’ modelin farklÄ± kodlama stillerine uyum saÄŸlamasÄ±nÄ± Ã¶ÄŸretir
Her iki datasetâ€™in solution alanÄ± eÄŸitimde kullanÄ±lmÄ±ÅŸtÄ±r.

EÄŸitim Pipelineâ€™Ä± (QLoRA)

EÄŸitimler Colab GPU Ã¼zerinde, aÅŸaÄŸÄ±daki adÄ±mlarla yapÄ±lmÄ±ÅŸtÄ±r:
Base model 4-bit quantization ile yÃ¼klendi.
LoRA katmanlarÄ± eklendi. (rank=64, alpha=16, dropout=0.1)
Datasetâ€™in solution alanÄ± tokenize edildi.

TrainingArguments:

logging_steps = 50
eval_steps = 100
save_steps = 100
num_train_epochs = 1
gradient_accumulation_steps = 4
learning_rate = 2e-4
EÄŸitim sÄ±rasÄ±nda her 100 adÄ±mda checkpoint oluÅŸturuldu
EÄŸitim sonunda LoRA adapterâ€™Ä± qwen2.5-*-lora-ckpt100 klasÃ¶rÃ¼ne kaydedildi

KullanÄ±lan Hyperparameterâ€™lar

LoRA Parametreleri
Parametre	DeÄŸer
Rank (r)	64
Alpha	16
Dropout	0.1
Target Modules	q_proj, k_proj, v_proj, o_proj
Bias	none
Training Parametreleri
Parametre	DeÄŸer
Learning Rate	2e-4
Batch Size	1
Gradient Accumulation	4
Effective Batch Size	4
Epoch	1
Max Seq Length	2048
Optimizer	paged_adamw_8bit
Scheduler	cosine
Warmup Ratio	0.03
Logging Steps	50
Eval Steps	100
Checkpoint Save Steps	100

Training Scripts

AÅŸaÄŸÄ±daki iki script GitHub iÃ§inde scripts/ klasÃ¶rÃ¼ndedir:
ğŸ”¹ train_deep.py
Deep dataset ile eÄŸitir
Model + tokenizer + LoRA yÃ¼kler
Checkpointâ€™leri her 100 adÄ±mda kaydeder
Final LoRA adapterâ€™Ä±nÄ± qwen2.5-deep-lora-ckpt100/ iÃ§ine yazar
ğŸ”¹ train_diverse.py
Diverse dataset ile eÄŸitir
AynÄ± pipeline, farklÄ± dataset
SonuÃ§lar qwen2.5-diverse-lora-ckpt100/ iÃ§ine kaydedilir
Her iki script, Colab notebookâ€™taki ile birebir aynÄ± davranÄ±ÅŸÄ± gÃ¶sterir.

Evaluation Script (Checkpoint SeÃ§imi)

Script yolu:
scripts/eval_checkpoints.py

YaptÄ±klarÄ±:
Deep & Diverse iÃ§in tÃ¼m checkpoint klasÃ¶rlerini okur
Ä°lk 100 Ã¶rnekten oluÅŸan test split ile evaluation yapar
Her checkpoint iÃ§in eval loss hesaplar
En iyi checkpointâ€™i otomatik seÃ§er
SonuÃ§larÄ± checkpoint_summary.json dosyasÄ±na kaydeder

Checkpoint Selection SonuÃ§larÄ±
Dataset	En iyi checkpoint	Eval Loss
DEEP	checkpoint-1250	â‰ˆ 0.4231
DIVERSE	checkpoint-1100	â‰ˆ 0.4734

Test Split PolitikasÄ±

Test datasÄ± asla eÄŸitimde kullanÄ±lmaz
Hem DEEP hem DIVERSE iÃ§in ilk 100 Ã¶rnek test seti olarak ayrÄ±ldÄ±
Checkpoint evaluation iÅŸlemi bu test split Ã¼zerinde yapÄ±ldÄ±
Bu, GÃ¶rev 4â€™Ã¼n gerekliliklerine birebir uygundur.


HuggingFace Modelleri (yÃ¼klendikten sonra)

Model linkleri buraya eklenecek:
ğŸ”¹ Deep LoRA Model
https://huggingface.co/<username>/qwen2.5-deep-lora
ğŸ”¹ Diverse LoRA Model
https://huggingface.co/<username>/qwen2.5-diverse-lora

EÄŸitim LoglarÄ±

TÃ¼m eÄŸitim loglarÄ± GitHubâ€™da logs/ klasÃ¶rÃ¼ndedir:

logs/deep_train.log
logs/diverse_train.log
logs/deep_eval.log
logs/diverse_eval.log


Bu loglar:
Train loss â†’ her 50 step
Eval loss â†’ her 100 step
Checkpoint oluÅŸumu â†’ her 100 step
bilgilerini iÃ§erir.

SonuÃ§

Bu proje:
QLoRA ile iki farklÄ± dataset Ã¼zerinde baÅŸarÄ±lÄ± ince ayar
Loss eÄŸrileri stabil ve anlamlÄ±
Checkpoint selection sÃ¼reciyle optimum performans garantisi
Modeller HuggingFaceâ€™e aktarÄ±labilir.
Scriptler ve tÃ¼m pipeline GitHubâ€™da reproducible ÅŸekilde yer alÄ±r.

Dosya YapÄ±sÄ±

qwen2.5-codegen-lora/
â”œâ”€ scripts/
â”‚  â”œâ”€ train_deep.py
â”‚  â”œâ”€ train_diverse.py
â”‚  â””â”€ eval_checkpoints.py
â”œâ”€ logs/
â”‚  â”œâ”€ deep_train.log
â”‚  â”œâ”€ diverse_train.log
â”‚  â”œâ”€ deep_eval.log
â”‚  â”œâ”€ diverse_eval.log
â”œâ”€ models/
â”œâ”€ notebook/
â”‚  â””â”€ Lora.ipynb
â”œâ”€ checkpoint_summary.json
â”œâ”€ requirements.txt
â””â”€ README.md
